{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f44a5083-ee39-4c97-b480-3004ef046afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "807ff067-21ec-4261-a1a7-f226eaed66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(filepath):\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        print(f\"Corrupted Image Found: {filepath}\", file=sys.stderr)\n",
    "        return False\n",
    "\n",
    "class FilteredImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(FilteredImageFolder, self).__init__(root=root, transform = transform)\n",
    "        valid_images = []\n",
    "\n",
    "        for item in self.imgs:\n",
    "            if is_valid_image(item[0]):\n",
    "                valid_images.append(item)\n",
    "        self.imgs = valid_images\n",
    "        self.samples = valid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6aa8cc-caa6-4d27-8223-3e468644eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_width, img_height)),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, translate=(0.2,0.2), scale=(0.8, 1.2), shear=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_width, img_height)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70112c17-53a2-407e-88e3-4fddb5f9d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Validating training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupted Image Found: dataset/train\\dogs\\11702.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and validating validation dataset...\n",
      "Training classes: ['cats', 'dogs']\n",
      "Validation classes: ['cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and Validating training dataset...\")\n",
    "train_dataset = FilteredImageFolder(\n",
    "    root='dataset/train',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(\"Loading and validating validation dataset...\")\n",
    "val_dataset = FilteredImageFolder(\n",
    "    root='dataset/validation',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "print(\"Training classes:\", train_dataset.classes)\n",
    "print(\"Validation classes:\", val_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb034803-bc63-45a7-9674-cd009804dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers = 0,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee835d8-94fe-42f1-ae7b-a1458fcf61c0",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b6d38ed-21dc-474d-bd91-00253d068871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64,128,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(128,128,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f4ef18c-404c-4905-847c-93f301a323bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(num_classes= len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e1e85-dcd1-4b4b-9890-12c5cc1391a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a061ab39143bcbc2471d668b13deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ai-ml-learning\\streamlit\\environment\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Norms:\n",
      "features.0.weight: 0.7935\n",
      "features.0.bias: 0.1984\n",
      "features.3.weight: 0.9426\n",
      "features.3.bias: 0.2497\n",
      "features.6.weight: 0.4788\n",
      "features.6.bias: 0.2531\n",
      "features.9.weight: 0.2229\n",
      "features.9.bias: 0.1569\n",
      "classifier.1.weight: 0.1418\n",
      "classifier.1.bias: 0.1957\n",
      "classifier.3.weight: 0.4177\n",
      "classifier.3.bias: 0.2848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12834a791c8641818369ce4879f235a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Loss: 0.6690, Train Acc: 59.19%\n",
      "Val Loss: 0.6747, Val Acc: 58.16%\n",
      "New best model saved! (Validation Accuracy: 58.16%)\n",
      "\n",
      "Epoch [2/20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b8100af14493ca8f84ccfeea2a356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Norms:\n",
      "features.0.weight: 0.2231\n",
      "features.0.bias: 0.0827\n",
      "features.3.weight: 0.2245\n",
      "features.3.bias: 0.0531\n",
      "features.6.weight: 0.0726\n",
      "features.6.bias: 0.0728\n",
      "features.9.weight: 0.0410\n",
      "features.9.bias: 0.0388\n",
      "classifier.1.weight: 0.0461\n",
      "classifier.1.bias: 0.0494\n",
      "classifier.3.weight: 0.0474\n",
      "classifier.3.bias: 0.0060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41961308c9224c6295a5fd98f3e6c1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Train Loss: 0.6360, Train Acc: 63.72%\n",
      "Val Loss: 0.6209, Val Acc: 66.33%\n",
      "New best model saved! (Validation Accuracy: 66.33%)\n",
      "\n",
      "Epoch [3/20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dccc1ab0a1450f9478a8dd9538b3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Modified training loop to track losses\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    pbar = tqdm(total=len(train_loader), desc=f'Training')\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs, labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norms = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norms[name] = param.grad.norm().item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_loss = train_loss / (i + 1)\n",
    "        acc = 100. * train_correct / train_total\n",
    "        pbar.set_description(f'Train | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%')\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Store average training loss\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    print(\"\\nGradient Norms:\")\n",
    "    for name, norm in grad_norms.items():\n",
    "        print(f\"{name}: {norm:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    pbar = tqdm(total=len(val_loader), desc=f'Validation')\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs, labels\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            avg_loss = val_loss / (i + 1)\n",
    "            acc = 100. * val_correct / val_total\n",
    "            pbar.set_description(f'Val | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%')\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "    # Store average validation loss\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f'\\nEpoch Summary:')\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'catsanddogs_classifier.pth')\n",
    "        print(f'New best model saved! (Validation Accuracy: {val_acc:.2f}%)')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff6a57-bc2c-42bc-bade-f4226f98d772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e737f-af39-480d-a48e-b4bd34cfef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b217751-b503-4510-9bb9-51cfea5f7f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18de10-b05f-4f6a-abbf-602403d9b125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c493764-88ba-4d1c-a4e3-ea77acf4b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba084c5-cd2b-47b5-bf7d-79713076c167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79720062-927c-4daa-914c-6cf6e04d82d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158aaba-59da-49ec-a62b-cd5547e7011b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e35c6-22d4-4d0b-9e5a-3bd50fcbc09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b8d84-dce3-4911-9e51-e9edc767815e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aedff-a7ff-4b3a-86eb-b690ffadac2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073406c3-ab56-4366-b991-91e19002100d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44966cc1-b597-4b3b-944f-9de9b7b9739d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
